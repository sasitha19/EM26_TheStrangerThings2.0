# -*- coding: utf-8 -*-
"""Eminence_Heart_Attack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u1YBMMlLp32yLGII8chl5KSLMgA97ISm
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load the training data
try:
    train_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Heart_Attack_training_dataset.csv")
    print("Training data loaded successfully.")
    print("Training data head:")
    display(train_df.head())
    #print("\nTraining data info:")
    #display(train_df.info())
except FileNotFoundError:
    print("Error: train.csv not found. Please make sure the file is in the correct directory.")

# Load the testing data
try:
    test_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Hear_Attack_evaluation_dataset.csv")
    print("\nTesting data loaded successfully.")
    print("Testing data head:")
    display(test_df.head())
    #print("\nTesting data info:")
    #display(test_df.info())
except FileNotFoundError:
    print("Error: test.csv not found. Please make sure the file is in the correct directory.")

"""## Evaluation and Prediction

## Model Selection and Training
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize the Random Forest Classifier
model = RandomForestClassifier(random_state=42)

# Train the model
model.fit(X_train_processed_df, y_train)

print("Model training complete.")

# Make predictions on the test data
if X_test_processed_df is not None:
    test_predictions = model.predict(X_test_processed_df)
    #print("\nTest Predictions:")
    #display(test_predictions)
else:
    print("\nTest data was not processed. Cannot make predictions.")

# Make predictions on the training data
y_train_pred = model.predict(X_train_processed_df)

# Evaluate the model on the training data
accuracy = accuracy_score(y_train, y_train_pred)
precision = precision_score(y_train, y_train_pred)
recall = recall_score(y_train, y_train_pred)
f1 = f1_score(y_train, y_train_pred)

print("Training Set Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

"""## Preprocessing"""

# Check for missing values
print("Missing values in training data:")
display(train_df.isnull().sum())

print("\nMissing values in testing data:")
if 'test_df' in globals():
    display(test_df.isnull().sum())
else:
    print("test_df is not defined. Please load the test data.")

"""There are no missing values in either dataset, so we can proceed with encoding and scaling."""

# Identify categorical and numerical columns
categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Exclude 'patient_id' and 'heart_attack_risk' from the lists
categorical_cols.remove('patient_id')
if 'heart_attack_risk' in numerical_cols:
    numerical_cols.remove('heart_attack_risk')

print("Categorical columns:", categorical_cols)
print("Numerical columns:", numerical_cols)

"""Let's inspect the unique values in the categorical columns to understand how to encode them."""

for col in categorical_cols:
    print(f"\nUnique values in {col}:")
    display(train_df[col].unique())

"""- 'sex' and 'diet' appear to be binary and can be label encoded.
- 'bp' is not in a standard numerical format and needs to be split and converted.
- 'country', 'continent', and 'hemisphere' are nominal and can be one-hot encoded.
"""

from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd # Import pandas

# Handle 'bp' column: Split into systolic and diastolic pressure
def split_bp(df):
    if 'bp' in df.columns:
        bp_split = df['bp'].str.split('/', expand=True)
        df['systolic_bp'] = pd.to_numeric(bp_split[0])
        df['diastolic_bp'] = pd.to_numeric(bp_split[1])
        df = df.drop('bp', axis=1)
    return df

# Ensure train_df and test_df are loaded before proceeding
if 'train_df' not in globals():
    print("Error: train_df not found. Please load the training data.")
elif 'test_df' not in globals():
    print("Error: test_df not found. Please load the testing data.")
else:
    train_df = split_bp(train_df)
    test_df = split_bp(test_df)

    # Update numerical and categorical columns after splitting 'bp'
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    if 'patient_id' in categorical_cols:
        categorical_cols.remove('patient_id')
    if 'heart_attack_risk' in numerical_cols:
        numerical_cols.remove('heart_attack_risk')
    if 'systolic_bp' not in numerical_cols:
        numerical_cols.append('systolic_bp')
    if 'diastolic_bp' not in numerical_cols:
        numerical_cols.append('diastolic_bp')


    # Remove duplicates from numerical_cols
    numerical_cols = list(dict.fromkeys(numerical_cols))


    print("Updated categorical columns:", categorical_cols)
    print("Updated numerical columns:", numerical_cols)

    # Separate features and target
    X_train = train_df.drop('heart_attack_risk', axis=1)
    y_train = train_df['heart_attack_risk']
    X_test = test_df.copy() # Test set has no target

    # Apply label encoding for 'sex' and 'diet'
    label_encoder_sex = LabelEncoder()
    label_encoder_diet = LabelEncoder()

    X_train['sex'] = label_encoder_sex.fit_transform(X_train['sex'])
    X_train['diet'] = label_encoder_diet.fit_transform(X_train['diet'])

    X_test['sex'] = label_encoder_sex.transform(X_test['sex'])
    X_test['diet'] = label_encoder_diet.transform(X_test['diet'])


    # Define preprocessing steps for scaling and one-hot encoding
    # Exclude 'sex' and 'diet' from one-hot encoding as they are already label encoded
    # Exclude 'patient_id' as it will be dropped
    features_to_onehot = [col for col in categorical_cols if col not in ['sex', 'diet']]
    features_to_scale = [col for col in numerical_cols if col not in ['sex', 'diet']]

    preprocessor = ColumnTransformer(
        transformers=[
            ('drop_id', 'drop', ['patient_id']),
            ('onehot', OneHotEncoder(handle_unknown='ignore'), features_to_onehot),
            ('scaler', StandardScaler(), features_to_scale)
        ],
        remainder='passthrough' # Keep other columns (like sex and diet)
    )

    # Apply the rest of the preprocessing steps (one-hot and scaling)
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)

    # Get feature names after one-hot encoding and scaling
    onehot_cols = preprocessor.named_transformers_['onehot'].get_feature_names_out(features_to_onehot)
    scaled_cols = features_to_scale
    # The remaining columns are 'sex' and 'diet' which were passed through
    remaining_cols = ['sex', 'diet']
    processed_cols = np.concatenate([onehot_cols, scaled_cols, remaining_cols])


    X_train_processed_df = pd.DataFrame(X_train_processed, columns=processed_cols)
    X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_cols)


    print("\nProcessed training data head:")
    display(X_train_processed_df.head())

    print("\nProcessed testing data head:")
    display(X_test_processed_df.head())

print(model)

import joblib
joblib.dump(model, "heart_attack_model.pkl")
loaded_model = joblib.load("heart_attack_model.pkl")

from google.colab import files
files.download("heart_attack_model.pkl")